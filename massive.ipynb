{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664ba3b-12f0-4381-8c7d-880a2f55ce5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcdfa0a-4b3b-414a-9e3f-bc70f332dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:43:46,923 [INFO] Descargando desde FTP: /v05/MSV000091530/updates/2023-03-23_glyanglife_fc206ce7/raw/global/Global_YXLC-G_cancer.raw  →  ./Massive/Global_YXLC-G_cancer.raw\n",
      "2025-09-19 11:44:21,228 [INFO] ✅ Archivo descargado: ./Massive/Global_YXLC-G_cancer.raw\n",
      "2025-09-19 11:44:21,432 [INFO] Ejecutando: mono /home/alejandro/Descargas/ThermoRawFileParser/ThermoRawFileParser.exe -i \"./Massive/Global_YXLC-G_cancer.raw\" -o \"./Massive/\" -f 1\n",
      "2025-09-19 11:44:21,778 [ERROR] Error ejecutando: mono /home/alejandro/Descargas/ThermoRawFileParser/ThermoRawFileParser.exe -i \"./Massive/Global_YXLC-G_cancer.raw\" -o \"./Massive/\" -f 1\n",
      "Command 'mono /home/alejandro/Descargas/ThermoRawFileParser/ThermoRawFileParser.exe -i \"./Massive/Global_YXLC-G_cancer.raw\" -o \"./Massive/\" -f 1' returned non-zero exit status 1.\n",
      "2025-09-19 11:44:21,778 [ERROR] Error procesando ./Massive/Global_YXLC-G_cancer.raw: Command 'mono /home/alejandro/Descargas/ThermoRawFileParser/ThermoRawFileParser.exe -i \"./Massive/Global_YXLC-G_cancer.raw\" -o \"./Massive/\" -f 1' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:44:21 INFO Started parsing ./Massive/Global_YXLC-G_cancer.raw\n",
      "2025-09-19 11:44:21 ERROR RAW file cannot be processed because of an error - ThermoFisher.CommonCore.RawFileReader.Facade.RawFileLoader\n",
      "2025-09-19 11:44:21 INFO Processing completed 1 errors, 0 warnings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import logging\n",
    "from ftplib import FTP\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "THERMO_RAW_PARSER = \"mono /home/alejandro/Descargas/ThermoRawFileParser/ThermoRawFileParser.exe\"\n",
    "MSGFPLUS = \"java -jar /home/alejandro/Descargas/MSGFPlus_v20240326/MSGFPlus.jar\"\n",
    "ID_FILE_CONVERTER = \"IDFileConverter\"\n",
    "FEATURE_FINDER = \"FeatureFinderIdentification\"\n",
    "PROTEIN_QUANTIFIER = \"ProteinQuantifier\"\n",
    "FALSE_DISCOVERY_RATE = \"FalseDiscoveryRate\"\n",
    "FASTA_DATABASE = \"human_proteome.fasta\"\n",
    "\n",
    "FTP_SERVER = \"massive-ftp.ucsd.edu\"\n",
    "BASE_DIR = \"./Massive/\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# Tu listado (el que subiste) renómbralo/ponlo aquí:\n",
    "LISTA_FTP = os.path.join(BASE_DIR, \"descarga_massive.txt\")\n",
    "\n",
    "CONTAMINANT_PROTEINS = [\n",
    "    \"sp|P04264|K2C1_HUMAN\",   # Keratina\n",
    "    \"sp|P35908|K22E_HUMAN\",   # Otra keratina\n",
    "]\n",
    "\n",
    "# ------------------ UTILIDADES ------------------\n",
    "def normalize_massive_path(p: str) -> str:\n",
    "    \"\"\"\n",
    "    Convierte rutas tipo 'f.MSV000091530/updates/...' en '/v05/MSV000091530/updates/...'\n",
    "    y limpia tabs/espacios extra.\n",
    "    \"\"\"\n",
    "    p = p.strip()\n",
    "    if not p:\n",
    "        return \"\"\n",
    "    # Quitar tabs/líneas con indentación\n",
    "    p = p.lstrip(\"\\t \").rstrip()\n",
    "    if p.startswith(\"f.MSV\"):\n",
    "        p = p.replace(\"f.MSV\", \"MSV\", 1)   # f.MSV000091530 -> MSV000091530\n",
    "        p = \"/v05/\" + p\n",
    "    # Asegurar que arranca por \"/\" para usar ftp.cwd sin sorpresas\n",
    "    if not p.startswith(\"/\"):\n",
    "        p = \"/\" + p\n",
    "    return p\n",
    "\n",
    "def read_remote_paths(list_file: str):\n",
    "    \"\"\"Lee las rutas del fichero y devuelve solo las que terminan en .raw normalizadas.\"\"\"\n",
    "    paths = []\n",
    "    with open(list_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            norm = normalize_massive_path(line)\n",
    "            if norm and norm.lower().endswith(\".raw\"):\n",
    "                paths.append(norm)\n",
    "    return paths\n",
    "\n",
    "def ftp_download_file(ftp: FTP, remote_path: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Descarga un archivo desde el FTP. Hace cwd al directorio y retrbinary del nombre base.\n",
    "    Crea BASE_DIR si no existe. Si ya existe localmente, lo salta.\n",
    "    \"\"\"\n",
    "    if os.path.exists(local_path):\n",
    "        logging.info(f\"Ya existe localmente, se omite descarga: {local_path}\")\n",
    "        return\n",
    "\n",
    "    remote_dir = os.path.dirname(remote_path)\n",
    "    remote_name = os.path.basename(remote_path)\n",
    "\n",
    "    logging.info(f\"Descargando desde FTP: {remote_path}  →  {local_path}\")\n",
    "    ftp.cwd(remote_dir)\n",
    "    with open(local_path, \"wb\") as fh:\n",
    "        ftp.retrbinary(f\"RETR {remote_name}\", fh.write)\n",
    "    logging.info(f\"✅ Archivo descargado: {local_path}\")\n",
    "\n",
    "def remove_contaminants(tsv_or_csv_file):\n",
    "    \"\"\"\n",
    "    Elimina proteínas contaminantes del CSV/TSV de cuantificación de proteínas.\n",
    "    ProteinQuantifier suele producir TSV con cabeceras; ajustamos lectura flexible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intento 1: TSV con cabeceras estándar (OpenMS suele usar \\t)\n",
    "        df = pd.read_csv(tsv_or_csv_file, sep=\"\\t\", comment=\"#\")\n",
    "    except Exception:\n",
    "        # Intento 2: CSV por si acaso\n",
    "        df = pd.read_csv(tsv_or_csv_file)\n",
    "\n",
    "    if \"protein\" not in df.columns:\n",
    "        # A veces OpenMS usa 'accession' o similar; intenta detectar\n",
    "        cand = [c for c in df.columns if c.lower() in (\"protein\", \"accession\", \"protein_accession\")]\n",
    "        if cand:\n",
    "            df = df.rename(columns={cand[0]: \"protein\"})\n",
    "        else:\n",
    "            logging.error(f\"No encuentro columna 'protein' (ni equivalente) en {tsv_or_csv_file}\")\n",
    "            return\n",
    "\n",
    "    df_filtrado = df[~df[\"protein\"].isin(CONTAMINANT_PROTEINS)]\n",
    "    # Conserva el mismo separador que tenía (si detectaste \\t, guarda como TSV)\n",
    "    if \"\\t\" in open(tsv_or_csv_file, \"r\", encoding=\"utf-8\", errors=\"ignore\").read(5000):\n",
    "        df_filtrado.to_csv(tsv_or_csv_file, sep=\"\\t\", index=False)\n",
    "    else:\n",
    "        df_filtrado.to_csv(tsv_or_csv_file, index=False)\n",
    "    logging.info(f\"Contaminantes eliminados en: {tsv_or_csv_file}\")\n",
    "\n",
    "def run_command(command):\n",
    "    logging.info(f\"Ejecutando: {command}\")\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Error ejecutando: {command}\\n{e}\")\n",
    "        raise\n",
    "\n",
    "def process_raw_file(local_raw_path: str):\n",
    "    try:\n",
    "        file_name = os.path.basename(local_raw_path)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # Salidas esperadas (contempla .mzML y .mzML.gz por si tu versión comprime)\n",
    "        mzml_file = os.path.join(BASE_DIR, f\"{base_name}.mzML\")\n",
    "        mzml_gz   = os.path.join(BASE_DIR, f\"{base_name}.mzML.gz\")\n",
    "\n",
    "        output_quant = os.path.join(BASE_DIR, f\"{base_name}_protein_abundances.csv\")\n",
    "        if os.path.exists(output_quant):\n",
    "            logging.info(f\"Salida existente; se omite: {output_quant}\")\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(local_raw_path):\n",
    "            logging.error(f\"No existe el .raw local: {local_raw_path}\")\n",
    "            return\n",
    "\n",
    "        # 1) RAW -> mzML  (IMPORTANTE: -o es carpeta, no archivo)\n",
    "        run_command(\n",
    "            f'{THERMO_RAW_PARSER} -i \"{local_raw_path}\" -o \"{BASE_DIR}\" -f 1'\n",
    "        )\n",
    "\n",
    "        # Si TRFP generó .mzML.gz, descomprime o ajusta la ruta\n",
    "        if os.path.exists(mzml_gz) and not os.path.exists(mzml_file):\n",
    "            # Opción A: usar el .mzML.gz directo si tus herramientas lo aceptan\n",
    "            # mzML para MS-GF+ debe ser descomprimido normalmente:\n",
    "            import gzip, shutil\n",
    "            with gzip.open(mzml_gz, \"rb\") as fin, open(mzml_file, \"wb\") as fout:\n",
    "                shutil.copyfileobj(fin, fout)\n",
    "            os.remove(mzml_gz)\n",
    "\n",
    "        if not os.path.exists(mzml_file):\n",
    "            logging.error(f\"No encuentro el mzML esperado: {mzml_file}\")\n",
    "            return\n",
    "\n",
    "        # 2) Identificación MS-GF+ (mzML -> mzid)\n",
    "        mzid_file = os.path.join(BASE_DIR, f\"{base_name}.mzid\")\n",
    "        run_command(f'{MSGFPLUS} -s \"{mzml_file}\" -d \"{FASTA_DATABASE}\" -o \"{mzid_file}\"')\n",
    "\n",
    "        # 3) .mzid -> .idXML\n",
    "        idxml_file = os.path.join(BASE_DIR, f\"{base_name}.idXML\")\n",
    "        run_command(f'{ID_FILE_CONVERTER} -in \"{mzid_file}\" -out \"{idxml_file}\"')\n",
    "\n",
    "        # 4) Cuantificación de péptidos (features)\n",
    "        feature_file = os.path.join(BASE_DIR, f\"{base_name}.featureXML\")\n",
    "        run_command(f'{FEATURE_FINDER} -in \"{mzml_file}\" -id \"{idxml_file}\" -out \"{feature_file}\"')\n",
    "\n",
    "        # 5) Cuantificación de proteínas\n",
    "        run_command(f'{PROTEIN_QUANTIFIER} -in \"{feature_file}\" -out \"{output_quant}\" -top:N 0')\n",
    "\n",
    "        # 6) Limpieza de contaminantes\n",
    "        remove_contaminants(output_quant)\n",
    "\n",
    "        # 7) Borrar intermedios\n",
    "        for p in [mzml_file, mzid_file, idxml_file, feature_file]:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                    logging.info(f\"Borrado: {p}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"No se pudo borrar {p}: {e}\")\n",
    "\n",
    "        logging.info(f\"✅ Procesado completo: {output_quant}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error procesando {local_raw_path}: {e}\")\n",
    "\n",
    "\n",
    "def download_all_raws():\n",
    "    \"\"\"Descarga todos los .raw listados en LISTA_FTP hacia BASE_DIR.\"\"\"\n",
    "    if not os.path.exists(LISTA_FTP):\n",
    "        logging.error(f\"No encuentro el listado: {LISTA_FTP}\")\n",
    "        return []\n",
    "\n",
    "    remote_paths = read_remote_paths(LISTA_FTP)\n",
    "    if not remote_paths:\n",
    "        logging.error(\"El listado no tenía rutas .raw válidas.\")\n",
    "        return []\n",
    "\n",
    "    local_paths = []\n",
    "    with FTP(FTP_SERVER) as ftp:\n",
    "        ftp.login()  # anónimo\n",
    "        for rp in remote_paths:\n",
    "            fname = os.path.basename(rp)\n",
    "            local = os.path.join(BASE_DIR, fname)\n",
    "            try:\n",
    "                ftp_download_file(ftp, rp, local)\n",
    "                local_paths.append(local)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Fallo descargando {rp}: {e}\")\n",
    "    return local_paths\n",
    "\n",
    "def main():\n",
    "    # 1) Descargar todos los RAW del listado (omite los ya existentes)\n",
    "    local_raws = download_all_raws()\n",
    "    if not local_raws:\n",
    "        logging.error(\"No hay .raw locales para procesar.\")\n",
    "        return\n",
    "\n",
    "    # 2) Procesar en paralelo\n",
    "    max_workers = min(3, len(local_raws))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        ex.map(process_raw_file, local_raws)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba08660d-e365-4d58-8b67-57f238750380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab584d-7023-4b00-bd61-72a1d2c02ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipython311",
   "language": "python",
   "name": "mipython311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
